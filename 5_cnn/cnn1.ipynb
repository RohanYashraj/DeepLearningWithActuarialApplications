{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Disclaimer\n",
                "This notebook was created for the SAV block course \"Deep Learning with Actuarial Applications in R\".\n",
                "\n",
                "The course is based on the publications on the following website: https://www.actuarialdatascience.org/\n",
                "\n",
                "Author: Daniel Meier\n",
                "\n",
                "# Convolutional Neural Networks for detection of distorted mortality rates due to errors and migration in population data\n",
                "# Abstract\n",
                "Convolutional Neural Networks (CNNs) are typically applied on image and video data for classification problems. A famous CNN winning the ImageNet Recognition Challenge is for example AlexNet, an 8-layer CNN for image classification. CNNs are also frequently applied in Computer Vision for object detection (existence and location) in images. This notebook shows how a simple 4-layer CNN (not counting the batch normalizations separately) can help to detect distortions of mortality rates (which can be considered as a 2D image with dimensions age and year and color channels for males, females, females less males, analogous to red, green, blue) due to errors and migration in population data.\n",
                "\n",
                "# Introduction\n",
                "Mortality rates $q_{x,t}$ by country and sex, i.e. the probably of dying at age $x$ (last birthday) between year $t$ and $t+1$, are derived from population numbers/exposures $E_{x,t}$ at given points in time $t$, e.g. from census data every 5 years, and death counts $D_{x,t}$, which typically are available at much better time resolution. Both $E_{x,t}$ and $D_{x,t}$ can be affected by errors and migration, which distorts mortality rates $q_{x,t}=D_{x,t}/E_{x,t}$.\n",
                "\n",
                "A way to measure this potential distortion is by considering normalized residuals $r_{x,t}$ defined as\n",
                "\n",
                "$$r_{x,t} = \\frac{E_{x,t}-E_{x-1,t-1}(1-q_{x-1,t-1})}{E_{x,t}}$$\n",
                "i.e. by comparing the actual population numbers/exposures $E_{x,t}$ to the one derived from the previous year $E_{x-1,t-1}(1-q_{x-1,t-1})$. A value $r_{x,t}>0$ indicates immigration or an error. A value $r_{x,t}<0$ indicates emigration or an error.\n",
                "\n",
                "This notebook applies a Computer Vision approach to all mortality rates available from the Human Mortality Database, where inputs $X$ are moving windows of size nx=10 times nt=10 and stepsize sx=5 and st=5 of (logit of) mortality rates $q_{x,t}$ for both sexes (males, females and the difference between females and males are used as channels, analogous to red/green/blue for color images), i.e. $X\\in\\mathbb{R}^{\\text{#windows}\\times 10\\times 10\\times 3}$ and outputs $Y\\in\\mathbb{R}^{\\text{#windows}}$ are maximum absolute values of $r_{x,t}$ over the same moving windows. Whenever the maxima of a given window exceeds the 95% quantile of maxima over all windows, we define that error/migration might be present in the given window.\n",
                "\n",
                "The trained CNN can then be used to detect areas/windows of $q_{x,t}$, where errors and migration potentially distorted mortality rates.\n",
                "\n",
                "The used CNN is a simple 3-layer network comprising\n",
                "\n",
                "* a convolutional 2D layer: 16 filters of size 3 times 3 and stepsize 1 and 1, 3 channels for logit mortality rates of males, females and females less males,\n",
                "* a convolutional 2D layer: 32 filters of size 3 times 3 and stepsize 1 and 1, 3 channels,\n",
                "* a convolutional 2D layer: 64 filters of size 3 times 3 and stepsize 1 and 1, 3 channels,\n",
                "* a fully connected layer.\n",
                "\n",
                "We first formulate the problem as a regression problem minimizing mean square errors, i.e. we would like to predict the size of errors and migration. Then, in order to assess the quality of resulting classifications we use area under curve (AUC).\n",
                "\n",
                "# 0. Import modules, definition of parameters\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "options(encoding = 'UTF-8')\n",
                "\n",
                "# Loading all the necessary packages\n",
                "library(\"repr\")  # not needed in the Rmarkdown version, only for Jupyter notebook\n",
                "library(\"abind\")\n",
                "library(\"pROC\")\n",
                "library(\"grid\")\n",
                "library(\"fields\")\n",
                "library(\"ggplot2\")\n",
                "library(\"plotly\")\n",
                "library(\"keras\")\n",
                "library(\"tensorflow\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "knitr::opts_chunk$set(fig.width = 9, fig.asp = 1)\n",
                "#options(repr.plot.width=4, repr.plot.height=10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pops <- c('AUS','AUT','BEL','BGR','BLR','CAN','CHE','CHL','CZE',\n",
                "        'DEU','DNK','ESP','EST','FIN','FRA','GBR','GRC','HKG',\n",
                "        'HRV','HUN','IRL','ISL','ISR','ITA','JPN','KOR','LTU',\n",
                "        'LUX','LVA','NLD','NOR','NZL','POL','PRT','RUS','SVK',\n",
                "        'SVN','SWE','TWN','UKR','USA')\n",
                "nx <- 10 # window size in terms of ages\n",
                "nt <- 10 # window size in terms of years\n",
                "sx <- 5 # step width of windows in terms of ages\n",
                "st <- 5 # step width of windows in terms of years\n",
                "minAge <- 21\n",
                "maxAge <- 80\n",
                "testRatio <- 0.15\n",
                "validationRatio <- 0.15\n",
                "thresholdQ <- 0.95 # defines migration/error in terms of a quantile threshold\n",
                "filterSize <- 5\n",
                "numberFilters <- 16\n",
                "filterSize1 <- 3\n",
                "numberFilters1 <- 16\n",
                "filterSize2 <- 3\n",
                "numberFilters2 <- 32\n",
                "filterSize3 <- 3\n",
                "numberFilters3 <- 64\n",
                "numberEpochs <- 800\n",
                "rxm <- list()\n",
                "rxf <- list()\n",
                "X <- list()\n",
                "Y <- list()\n",
                "dataRoot <- \"../../data\"\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Load and visualize mortality rates of GBR males, ages 0 to 110, years 1922 to 2016.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "qxm <- as.matrix(read.csv(file.path(dataRoot, \"cnn1\", \"GBR_M.txt\"), skip = 1, sep = \"\", header = TRUE))\n",
                "knitr::kable(head(qxm))\n",
                "\n",
                "fig <- plot_ly(z = matrix(as.numeric(qxm[, 4]), nrow = 111)[1:110, ]) %>%\n",
                "        layout(title = 'Mortality rates GBR males', scene = list(\n",
                "          xaxis = list(title = 'Year'),\n",
                "          yaxis = list(title = 'Age'),\n",
                "          zaxis = list(title = 'qx')\n",
                "        )) %>%\n",
                "        add_surface()\n",
                "fig\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Load and visualize exposures E, i.e. population numbers by age and year of GBR males.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "E <- as.matrix(read.csv(file.path(dataRoot, \"cnn1\", \"GBR.txt\"), skip = 1, sep = \"\", header = TRUE))\n",
                "knitr::kable(head(E))\n",
                "\n",
                "fig <- plot_ly(z = matrix(as.numeric(E[, 4]), nrow = 111)[1:110, ]) %>%\n",
                "        layout(title = 'Exposures GBR males', scene = list(\n",
                "          xaxis = list(title = 'Year'),\n",
                "          yaxis = list(title = 'Age'),\n",
                "          zaxis = list(title = 'E')\n",
                "        )) %>%\n",
                "        add_surface()\n",
                "fig\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The preparation of the model inputs X, the set of 10x10x3 \"images\", as well as the preparation of the model outputs Y, the set of residuals, can be looked up in detail in the online tutorial at https://github.com/JSchelldorfer/ActuarialDataScience/blob/master/9%20-%20Convolutional%20neural%20network%20case%20studies/cnn1.ipynb. For this course, we skip this step and directly load the results of these preparations.\n",
                "\n",
                "The plots show the outputs Y rearranged into age (x-axis) times years buckets (y-axis).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for (jPop in 1:length(pops)) {\n",
                "    pop = pops[jPop]\n",
                "    \n",
                "    lqxm = as.matrix(read.csv(paste0(dataRoot, \"/cnn1/logit_qx_\", pops[jPop], \"_m.csv\"), sep = \",\", header = FALSE))\n",
                "    lqxf = as.matrix(read.csv(paste0(dataRoot, \"/cnn1/logit_qx_\", pops[jPop], \"_f.csv\"), sep = \",\", header = FALSE))\n",
                "       \n",
                "    rxm[[pop]] = as.matrix(read.csv(paste0(dataRoot, \"/cnn1/residuals_\", pops[jPop], \"_m.csv\"), sep = \",\", header = FALSE))\n",
                "    rxf[[pop]] = as.matrix(read.csv(paste0(dataRoot, \"/cnn1/residuals_\", pops[jPop], \"_f.csv\"), sep = \",\", header = FALSE))\n",
                "    \n",
                "    if (is.element(pop, c('JPN','RUS','USA'))) {\n",
                "        image.plot(t(rxm[[pop]]))\n",
                "        mtext(line = 2, side = 1, paste(pop, 'males'))\n",
                "    }\n",
                "    \n",
                "    mx <- floor(floor((maxAge - minAge + 1 - nx) / sx + 1))\n",
                "    mt <- floor(floor((nrow(rxm[[pop]]) - nt) / st + 1))\n",
                "    \n",
                "    X[[pop]] <- array(0, dim = c(mx * mt, nx, nt, 3))\n",
                "    Y[[pop]] <- array(0, dim = c(mx * mt))\n",
                "    \n",
                "    for (j in 0:(mx-1)) {\n",
                "        for (k in 0:(mt-1)) {                                \n",
                "            # set up logit qx windows of size nt x nx for each population as input X\n",
                "            # (population x year buckets x age buckets x sex)\n",
                "            # logit qx of males as first channel:\n",
                "            X[[pop]][k*mx+j+1, , , 1] <- lqxm[(k*st+1):(k*st+nt), (j*sx+1):(j*sx+nx)]\n",
                "            # logit qx of females as second channel:\n",
                "            X[[pop]][k*mx+j+1, , , 2] <- lqxf[(k*st+1):(k*st+nt), (j*sx+1):(j*sx+nx)]\n",
                "            # logit qx of females less males as third channel:\n",
                "            X[[pop]][k*mx+j+1, , , 3] <- X[[pop]][(k*mx+j+1), , , 2] - X[[pop]][k*mx+j+1, , , 1]\n",
                "            # define output Y as the maximum absolute value of normalized residuals over each window of size nt x nx\n",
                "            Y[[pop]][k*mx+j+1] <- max(0.5 * abs(\n",
                "              rxm[[pop]][(k*st+1):(k*st+nt), (j*sx+1):(j*sx+nx)] + rxf[[pop]][(k*st+1):(k*st+nt), (j*sx+1):(j*sx+nx)]\n",
                "            ))\n",
                "        }\n",
                "    }\n",
                "}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# normalize X, Y\n",
                "for (pop in pops) {\n",
                "    minX1 <- min(X[[pop]][,,,1])\n",
                "    maxX1 <- max(X[[pop]][,,,1])\n",
                "    minX2 <- min(X[[pop]][,,,2])\n",
                "    maxX2 <- max(X[[pop]][,,,2])\n",
                "    minX3 <- min(X[[pop]][,,,3])\n",
                "    maxX3 <- max(X[[pop]][,,,3])\n",
                "    minY <- min(Y[[pop]])\n",
                "    maxY <- max(Y[[pop]])    \n",
                "    X[[pop]][,,,1] <- (X[[pop]][,,,1] - minX1) / (maxX1 - minX1)\n",
                "    X[[pop]][,,,2] <- (X[[pop]][,,,2] - minX2) / (maxX2 - minX2)\n",
                "    X[[pop]][,,,3] <- (X[[pop]][,,,3] - minX3) / (maxX3 - minX3)\n",
                "    Y[[pop]] <- (Y[[pop]] - minY) / (maxY - minY)\n",
                "}\n",
                "grid.newpage()\n",
                "grid.raster(X[['GBR']][2,,,], interpolate = FALSE)  # plot as RBG image\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 1. Setup and train CNN on a selected subset of all populations\n",
                "\n",
                "The full set of countries is quite heterogenous in terms of immigration/error structures. Observe for example the horizontal, diagonal, and vertical structures of residuals for Japan, Russia, USA above. In the following, we work on the largest cluster of countries with similar immigration/error structure and skip how this cluster was derived. For more details see https://github.com/JSchelldorfer/ActuarialDataScience/blob/master/9%20-%20Convolutional%20neural%20network%20case%20studies/cnn1.ipynb\n",
                "\n",
                "**Exercise:** Use other selections of countries and compare AUCs. (Keep an eye on the number of input samples to be sufficiently large.)\n",
                "\n",
                "**Exercise:** Experiment with other structures/parameters of the CNN, e.g. change the number of layers, strides parameters, activation functions, etc. Make use of summary(cnn) to check the dimensions of inputs/outputs of each layer. How are the dimensions affected by strides, padding, kernel_size, number of filters?\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "selectedPop <- c('AUS', 'BGR', 'BLR', 'CAN', 'CZE', 'ESP', 'EST', 'FIN',\n",
                "          'GBR', 'GRC', 'HKG', 'ISL', 'ITA', 'JPN', 'LTU', 'NZL',\n",
                "          'POL', 'PRT', 'RUS', 'SVK', 'TWN', 'UKR')\n",
                "\n",
                "allX <- array(numeric(), c(0,10,10,3))\n",
                "allY <- array(numeric(), c(0))\n",
                "for (kPop in selectedPop) {\n",
                "    allX <- abind(allX, X[[kPop]], along = 1)\n",
                "    allY <- abind(allY, Y[[kPop]], along = 1)\n",
                "}\n",
                "\n",
                "set.seed(0)\n",
                "tf$random$set_seed(0)\n",
                "\n",
                "testIdx <- runif(length(allY)) < testRatio\n",
                "testX <- allX[testIdx,,,]\n",
                "testY <- allY[testIdx]\n",
                "trainX <- allX[!testIdx,,,]\n",
                "trainY <- allY[!testIdx]\n",
                "\n",
                "cnn <- keras_model_sequential() %>% \n",
                "  layer_batch_normalization() %>%\n",
                "  layer_conv_2d(filters = numberFilters, kernel_size = c(filterSize1, filterSize1),\n",
                "              strides = c(1,1), padding = 'valid', data_format = 'channels_last') %>% \n",
                "  layer_batch_normalization() %>%\n",
                "  layer_activation('relu') %>%\n",
                "  layer_conv_2d(filters = numberFilters, kernel_size = c(filterSize2, filterSize2),\n",
                "                strides = c(1,1), padding = 'valid', data_format = 'channels_last') %>% \n",
                "  layer_batch_normalization() %>%\n",
                "  layer_activation('relu') %>%\n",
                "  layer_conv_2d(filters = numberFilters, kernel_size = c(filterSize3, filterSize3),\n",
                "                strides = c(1,1), padding = 'valid', data_format = 'channels_last') %>% \n",
                "  layer_batch_normalization() %>%\n",
                "  layer_activation('relu') %>%\n",
                "  layer_flatten() %>%\n",
                "  layer_dense(1) %>%\n",
                "  layer_activation('sigmoid') %>%\n",
                "  compile(loss = 'mean_squared_error', optimizer = 'sgd')\n",
                "\n",
                "summary <- cnn %>% fit(\n",
                "  x = trainX,\n",
                "  y = trainY,\n",
                "  epochs = numberEpochs / 4,\n",
                "  validation_split = validationRatio,\n",
                "  sample_weight = (0.2 + trainY) / 1.2,\n",
                "  batch_size = 64,\n",
                "  verbose = 0\n",
                ")\n",
                "\n",
                "plot(summary)\n",
                "\n",
                "migErr <- testY >= quantile(testY, thresholdQ)\n",
                "testPred <- predict(cnn, testX)\n",
                "\n",
                "plot(testPred, testY - testPred[, 1], col = migErr + 5, main = 'Test set of combined populations',\n",
                "     xlab = 'Prediction P', ylab = 'Residuals Y-P')\n",
                "plot(testPred, testY, col = migErr + 5, main = 'Test set of combined populations',\n",
                "     xlab = 'Prediction P', ylab = 'Output Y')\n",
                "\n",
                "rocobj <- plot.roc(1 * migErr, testPred[, 1], main = \"ROC, AUC\", ci = TRUE, print.auc = TRUE)\n",
                "ciobj <- ci.se(rocobj, specificities = seq(0, 1, 0.01))\n",
                "plot(ciobj, type = \"shape\")\n",
                "plot(ci(rocobj, of = \"thresholds\", thresholds = \"best\"))\n",
                "summary(cnn)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Comparing predictions P and outputs Y.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "allPred <- predict(cnn, allX)\n",
                "df <- setNames(data.frame(\n",
                "        rep(1:(length(allY)/11), each = 11),\n",
                "        rep(1:11, length(allY)/11),\n",
                "        allY,\n",
                "        allPred[, 1],\n",
                "        allY - allPred[, 1]\n",
                "      ), c('x','y','z1','z2','z3'))\n",
                "\n",
                "ggplot(df, aes(y, x, fill = z1)) + geom_tile() +\n",
                "  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0) +\n",
                "  ggtitle('Output Y') + xlab('Age buckets') + ylab('Years/countries')\n",
                "\n",
                "ggplot(df, aes(y, x, fill = z2)) + geom_tile() +\n",
                "  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0) +\n",
                "  ggtitle('Prediction P') + xlab('Age buckets') + ylab('Years/countries')\n",
                "\n",
                "ggplot(df, aes(y, x, fill = z3)) + geom_tile() +\n",
                "  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0) +\n",
                "  ggtitle('Residuals Y-P') + xlab('Age buckets') + ylab('Years/countries')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
