{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Disclaimer\n",
                "This notebook was created for the SAV block course \"Deep Learning with Actuarial Applications in R\".\n",
                "\n",
                "The course is based on the publications on the following website: https://www.actuarialdatascience.org/\n",
                "\n",
                "Author: Daniel Meier\n",
                "\n",
                "# Applying Convolutional Neural Networks for image classification\n",
                "\n",
                "Inception v3 is one of the pre-trained neural networks available in the keras library. Developed by Google, Inception v3 has 23'851'784 parameters, uses 299x299x3 color images as inputs, and has 1'000 image classes, e.g. baseketball, violin, ear, etc.\n",
                "\n",
                "## 0. Load the model, classes, define functions\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import json\n",
                "\n",
                "path_to_files = '../../data/cnn3/'\n",
                "model = tf.keras.applications.InceptionV3(\n",
                "    include_top=True,\n",
                "    weights=path_to_files + 'inception_v3_weights_tf_dim_ordering_tf_kernels.h5',\n",
                "    input_tensor=None,\n",
                "    input_shape=None,\n",
                "    pooling=None,\n",
                "    classes=1000\n",
                ")\n",
                "\n",
                "with open(path_to_files + 'imagenet_class_index.json') as file:\n",
                "    classes = json.load(file)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Number of layers of Inception v3:\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "len(model.layers)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The complete structure of Inception v3 is quite involved...\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.summary()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "1'000 image classes:\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print([x[1] for x in classes.values()])\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The function `predict` returns the full 1'000 dimensional output vector of a 299x299x3 input image.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from keras.preprocessing import image\n",
                "from keras.applications.inception_v3 import preprocess_input\n",
                "\n",
                "def predict(model, img, target_size=(299, 299)):\n",
                "    x = image.img_to_array(img)\n",
                "    x = np.expand_dims(x, axis=0)\n",
                "    x = preprocess_input(x)\n",
                "    pred = model.predict(x)\n",
                "    return pred\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In the following we will be using 2 methods to obtain a 299x299x3 array/image, both methods can make use of a built-in camera if available.\n",
                "\n",
                "* **Method 1:** Taking screenshots every 2 seconds and taking a 299x299 region from the screenshot as input. Note that you can use the parameters `x_shift` and `y_shift` to move this region. With the current setting the region should be located near the upper left corner of your screen.\n",
                "\n",
                "* **Method 2:** In case a built-in camera is available, this method extracts a snapshot directly from the video stream. No calibration of parameters (`x_shift`, `y_shift`) needed.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from PIL import Image\n",
                "import base64\n",
                "import io\n",
                "import time\n",
                "from IPython.display import clear_output\n",
                "import matplotlib.pylab as plt\n",
                "\n",
                "# getting the top 5 classes based on a 299x299x3 array/image\n",
                "def get_classes(img):\n",
                "    pred = predict(model, img)[0]\n",
                "    idx = (-pred).argsort()[:5]\n",
                "    msg = ''\n",
                "    for i in idx:    \n",
                "        msg += (str(pred[i])+\": \"+classes[str(i)][1]+\"\\n\")\n",
                "    return msg\n",
                "\n",
                "# getting the top 5 classes based on a PIL image\n",
                "def get_classes_pil(pil_img):\n",
                "    pil_img = Image.open(io.BytesIO(base64.b64decode(pil_img.split(',')[1])))\n",
                "    img = np.array(pil_img)\n",
                "    img = img[:,:,0:3]\n",
                "    return get_classes(img)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Method 1\n",
                "\n",
                "\n",
                "First, calibrate parameters x_shift and y_shift such that e.g. the video stream is captured.\n",
                "Then, in order to stop the loop hit **interrupt kernel**.\n",
                "\n",
                "**Exercise:** Calibrate `x_shift` and `y_shift` parameters. Uncomment the `#break` line to obtain a plot of the captured region. Then you can also try to find some images on google and put them over the captured region to get them classified...\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython.display import HTML\n",
                "\n",
                "HTML(\"\"\"\n",
                "<video id=\"video\" width=\"400\" height=\"300\" autoplay><\/video>\n",
                "<canvas id=\"canvas\" width=\"299\" height=\"299\"><\/canvas>\n",
                "<script>\n",
                "var video = document.getElementById('video');\n",
                "if(navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {\n",
                "    navigator.mediaDevices.getUserMedia({ video: true }).then(function(stream) {\n",
                "        video.srcObject=stream;\n",
                "        video.play();\n",
                "    });\n",
                "}\n",
                "<\/script>\n",
                "\"\"\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pyautogui\n",
                "\n",
                "x_shift = 550\n",
                "y_shift = 250\n",
                "while True:    \n",
                "    pil_img = pyautogui.screenshot()\n",
                "    img = np.array(pil_img)\n",
                "    img = img[y_shift:y_shift+299,x_shift:x_shift+299,0:3]\n",
                "    clear_output()\n",
                "    print(get_classes(img))\n",
                "    plt.imshow(img)\n",
                "    time.sleep(2)\n",
                "    #break\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Method 2\n",
                "\n",
                "* First cell: extract image from video stream into img variable\n",
                "* Second cell: print (top 5) predictions\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython.display import Javascript, display\n",
                "js = \"\"\"\n",
                "    var canvas = document.getElementById('canvas');\n",
                "    var context = canvas.getContext('2d');\n",
                "    var video = document.getElementById('video');\n",
                "    context.drawImage(video, -50, 0, 400, 300);\n",
                "    var myCanvas = document.getElementById('canvas');\n",
                "    var image = myCanvas.toDataURL(\"image/png\");    \n",
                "    IPython.notebook.kernel.execute(\"img = '\" + image + \"'\")    \n",
                "    \"\"\"\n",
                "display(Javascript(js))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "display(Image.open(io.BytesIO(base64.b64decode(img.split(',')[1]))))\n",
                "print(get_classes_pil(img))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
