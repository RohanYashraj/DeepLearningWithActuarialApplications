{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Disclaimer\n",
                "\n",
                "**General disclaimer**\n",
                "\n",
                "This notebook was created for the course \"Deep Learning with Actuarial Applications in R\" (15th and 16th October in Zurich) of the Swiss Association of Actuaries (https://www.actuaries.ch/).\n",
                "\n",
                "The course is based on the publications (article, code and data) on the following website: https://www.actuarialdatascience.org/.\n",
                "\n",
                "Authors are Daniel Meier and JÃ¼rg Schelldorfer, with support from Mirai Solutions GmbH (https://mirai-solutions.ch/).\n",
                "\n",
                "**Notebook-specific disclaimer**\n",
                "\n",
                "This notebook closely follows:\n",
                "\n",
                "* https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3441030\n",
                "* https://github.com/JSchelldorfer/ActuarialDataScience/tree/master/6%20-%20Lee%20and%20Carter%20go%20Machine%20Learning%20Recurrent%20Neural%20Networks\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "options(encoding = 'UTF-8')\n",
                "\n",
                "options(keras.view_metrics = FALSE)\n",
                "\n",
                "# Loading all the necessary packages\n",
                "# if (!require(data.table))\n",
                "#   install.packages(\"data.table\")\n",
                "# if (!require(dplyr))\n",
                "#   install.packages(\"dplyr\")\n",
                "# if (!require(ggplot2))\n",
                "#   install.packages(\"ggplot2\")\n",
                "# if (!require(reshape2))\n",
                "#   install.packages(\"reshape2\")\n",
                "# if (!require(stringr))\n",
                "#   install.packages(\"stringr\")\n",
                "# if (!require(keras))\n",
                "#   install.packages(\"keras\")\n",
                "# if (!require(forecast))\n",
                "#   install.packages(\"forecast\")\n",
                "\n",
                "require(data.table)\n",
                "require(dplyr)\n",
                "require(ggplot2)\n",
                "require(reshape2)\n",
                "require(stringr)\n",
                "require(keras)\n",
                "require(forecast)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Checking package versions\n",
                "packageVersion('keras')               # version keras 2.3.0.0\n",
                "packageVersion('tensorflow')          # version tensorflow 2.2.0 (TensorFlow backend > 2.0 and Python 3.7)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# set seed to obtain best reproducibility. note that the underlying architecture may affect results nonetheless, so full reproducibility cannot be guaranteed across different platforms.\n",
                "seed <- 100\n",
                "Sys.setenv(PYTHONHASHSEED = seed)\n",
                "set.seed(seed)\n",
                "reticulate::py_set_seed(seed)\n",
                "tensorflow::tf$random$set_seed(seed)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sessionInfo()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##################################################################\n",
                "#### Load data\n",
                "##################################################################\n",
                "\n",
                "all_mort <- fread(\"CHE_mort.csv\")\n",
                "all_mort$Gender <- as.factor(all_mort$Gender)\n",
                "\n",
                "# The data has been downloaded from the Human Mortality Database (HMD).\n",
                "# We have applied some pre-processing to this data so that Lee-Carter can be applied.\n",
                "# Values that differ from the HMD have received a flag in the csv file\n",
                "\n",
                "str(all_mort)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##################################################################\n",
                "#### Heatmap of selected data\n",
                "##################################################################\n",
                "\n",
                "gender <- \"Male\"\n",
                "#gender <- \"Female\"\n",
                "m0 <- c(min(all_mort$logmx), max(all_mort$logmx))\n",
                "# rows are calendar year t, columns are ages x\n",
                "logmx <- t(matrix(\n",
                "           as.matrix(all_mort[which(all_mort$Gender == gender), \"logmx\"]),\n",
                "           nrow = 100,\n",
                "           ncol = nrow(all_mort) / 2 / 100\n",
                "         ))\n",
                "image(z = logmx,\n",
                "      useRaster = TRUE,\n",
                "      zlim = m0,\n",
                "      col = rev(rainbow(n = 60, start = 0, end = .72)),\n",
                "      xaxt = 'n',\n",
                "      yaxt = 'n',\n",
                "      main = paste(\"Swiss \", gender, \" raw log-mortality rates\", sep = \"\"),\n",
                "      ylab = \"age x\",\n",
                "      xlab = \"calendar year t\"\n",
                ")\n",
                "axis(1, at = c(0:(2016 - 1950)) / (2016 - 1950), c(1950:2016))\n",
                "axis(2, at = c(0:49) / 50, labels = c(0:49) * 2)\n",
                "lines(x = rep((1999 - 1950) / (2016 - 1950), 2),\n",
                "      y = c(0:1),\n",
                "      lwd = 2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gender <- \"Male\"\n",
                "\n",
                "#gender <- \"Female\"\n",
                "x <- 50\n",
                "t <- 1999\n",
                "logmx <- t(matrix(\n",
                "            as.matrix(all_mort[which(all_mort$Gender == gender), \"logmx\"]),\n",
                "            nrow = 100,\n",
                "            ncol = nrow(all_mort) / 2 / 100\n",
                "         ))\n",
                "mx <- t(matrix(as.matrix(all_mort[which(all_mort$Gender == gender), \"mx\"]),\n",
                "               nrow = 100,\n",
                "               ncol = nrow(all_mort) / 2 / 100\n",
                "      ))\n",
                "par(mfrow = c(2, 2))\n",
                "plot(x = min(all_mort$Year):max(all_mort$Year),\n",
                "     y = logmx[, x + 1],\n",
                "     xlab = \"calendar years\",\n",
                "     ylab = \"raw log-mortality rates\",\n",
                "     main = paste(\"Swiss \", gender, \", \", x, \" year old\", sep = \"\"),\n",
                "     type = 'l',\n",
                "     col = \"blue\"\n",
                ")\n",
                "plot(x = min(all_mort$Year):max(all_mort$Year),\n",
                "     y = mx[, x + 1],\n",
                "     xlab = \"calendar years\",\n",
                "     ylab = \"raw mortality rates\",\n",
                "     main = paste(\"Swiss \", gender, \", \", x, \" year old\", sep = \"\"),\n",
                "     type = 'l',\n",
                "     col = \"blue\"\n",
                ")\n",
                "plot(x = min(all_mort$Age):max(all_mort$Age),\n",
                "     y = logmx[t - min(all_mort$Year) + 1, ],\n",
                "     xlab = \"age\",\n",
                "     ylab = \"raw log-mortality rates\",\n",
                "     main = paste(\"Swiss \", gender, \", year \", t, sep = \"\"),\n",
                "     type = 'l',\n",
                "     col = \"blue\"\n",
                ")\n",
                "plot(x = min(all_mort$Age):max(all_mort$Age),\n",
                "     y = mx[t - min(all_mort$Year) + 1, ],\n",
                "     xlab = \"age\",\n",
                "     ylab = \"raw mortality rates\",\n",
                "     main = paste(\"Swiss \", gender, \", year \", t, sep = \"\"),\n",
                "     type = 'l',\n",
                "     col = \"blue\"\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##################################################################\n",
                "#### Fit Lee Carter to the selected country \n",
                "#### and extrapolate with Random Walk with Drift\n",
                "##################################################################\n",
                "\n",
                "# log m_tx = a_x + b_x*k_t\n",
                "\n",
                "ObsYear <- 1999\n",
                "gender <- \"Male\"\n",
                "train <- all_mort[Year <= ObsYear][Gender == gender]\n",
                "min(train$Year)\n",
                "\n",
                "### fit via SVD\n",
                "train[, ax := mean(logmx), by = (Age)]\n",
                "train[, mx_adj := logmx - ax]\n",
                "rates_mat <- as.matrix(train %>% \n",
                "                dcast.data.table(Age ~ Year, value.var = \"mx_adj\", sum))[, -1]\n",
                "svd_fit <- svd(rates_mat)\n",
                "\n",
                "ax <- train[, unique(ax)] # take care of potential duplicates???\n",
                "bx <- svd_fit$u[, 1] * svd_fit$d[1]\n",
                "kt <- svd_fit$v[, 1]\n",
                "\n",
                "c1 <- mean(kt)\n",
                "c2 <- sum(bx)\n",
                "ax <- ax + c1 * bx\n",
                "bx <- bx / c2\n",
                "kt <- (kt - c1) * c2\n",
                "\n",
                "### extrapolation and forecast\n",
                "test  <- all_mort[Year > ObsYear][Gender == gender]\n",
                "t_forecast <- test[, unique(Year)] %>% length()\n",
                "forecast_kt  = kt %>% forecast::rwf(t_forecast, drift = T)\n",
                "kt_forecast = forecast_kt$mean\n",
                "\n",
                "\n",
                "##################################################################\n",
                "#### Illustrate and back-test Lee-Carter prediction\n",
                "##################################################################\n",
                "\n",
                "# illustration selected drift\n",
                "plot_data <- c(kt, kt_forecast)\n",
                "plot(plot_data,\n",
                "     pch = 20,\n",
                "     col = \"red\",\n",
                "     cex = 2,\n",
                "     cex.lab = 1.5,\n",
                "     xaxt = 'n',\n",
                "     ylab = \"values k_t\",\n",
                "     xlab = \"calendar year t\",\n",
                "     main = list(paste(\"estimated process k_t for \", gender, sep = \"\"), \n",
                "                 cex = 1.5)\n",
                ")\n",
                "points(kt, col = \"blue\", pch = 20, cex = 2)\n",
                "axis(1,\n",
                "     at = c(1:length(plot_data)),\n",
                "     labels = c(1:length(plot_data)) + 1949)\n",
                "abline(v = (length(kt) + 0.5), lwd = 2)\n",
                "\n",
                "\n",
                "# in-sample and out-of-sample analysis\n",
                "fitted_train = (ax + (bx) %*% t(kt)) %>% melt\n",
                "train$pred_LC_svd = fitted_train$value %>% exp\n",
                "fitted_test = (ax + (bx) %*% t(kt_forecast)) %>% melt\n",
                "test$pred_LC_svd =   fitted_test$value %>% exp\n",
                "round(c((mean((train$mx - train$pred_LC_svd) ^ 2) * 10 ^ 4) , \n",
                "        (mean((test$mx - test$pred_LC_svd) ^ 2) * 10 ^ 4)), 4)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##################################################################\n",
                "### Designing an LSTMs\n",
                "##################################################################\n",
                "\n",
                "# single hidden LSTM layer\n",
                "LSTM1 <- function(T0, tau0, tau1, y0 = 0, optimizer) {\n",
                "  Input <- layer_input(shape = c(T0, tau0),\n",
                "                       dtype = 'float32',\n",
                "                       name = 'Input')\n",
                "  Output <- Input %>%\n",
                "    layer_lstm(units = tau1,\n",
                "               activation = 'tanh',\n",
                "               recurrent_activation = 'tanh',\n",
                "               name = 'LSTM1'\n",
                "    ) %>%\n",
                "    layer_dense(units = 1,\n",
                "                activation = k_exp,\n",
                "                name = \"Output\",\n",
                "                weights = list(array(0, dim = c(tau1, 1)), \n",
                "                               array(log(y0), dim = c(1)))\n",
                "    )\n",
                "  model <- keras_model(inputs = list(Input), outputs = c(Output))\n",
                "  model %>% compile(loss = 'mean_squared_error', optimizer = optimizer)\n",
                "}\n",
                "\n",
                "# double hidden LSTM layers\n",
                "LSTM2 <- function(T0, tau0, tau1, tau2, y0 = 0, optimizer) {\n",
                "  Input <- layer_input(shape = c(T0, tau0),\n",
                "                       dtype = 'float32',\n",
                "                       name = 'Input')\n",
                "  Output <- Input %>%\n",
                "    layer_lstm(units = tau1,\n",
                "               activation = 'tanh',\n",
                "               recurrent_activation = 'tanh',\n",
                "               return_sequences = TRUE,\n",
                "               name = 'LSTM1'\n",
                "    ) %>%\n",
                "    layer_lstm(units = tau2,\n",
                "               activation = 'tanh',\n",
                "               recurrent_activation = 'tanh',\n",
                "               name = 'LSTM2'\n",
                "    ) %>%\n",
                "    layer_dense(units = 1,\n",
                "                activation = k_exp,\n",
                "                name = \"Output\",\n",
                "                weights = list(array(0, dim = c(tau2, 1)), \n",
                "                               array(log(y0), dim = c(1)))\n",
                "    )\n",
                "  model <- keras_model(inputs = list(Input), outputs = c(Output))\n",
                "  model %>% compile(loss = 'mean_squared_error', optimizer = optimizer)\n",
                "}\n",
                "\n",
                "# triple hidden LSTM layers\n",
                "LSTM3 <- function(T0, tau0, tau1, tau2, tau3, y0 = 0, optimizer) {\n",
                "  Input <- layer_input(shape = c(T0, tau0),\n",
                "                       dtype = 'float32',\n",
                "                       name = 'Input')\n",
                "  Output <- Input %>%\n",
                "    layer_lstm(units = tau1,\n",
                "               activation = 'tanh',\n",
                "               recurrent_activation = 'tanh',\n",
                "               return_sequences = TRUE,\n",
                "               name = 'LSTM1'\n",
                "    ) %>%\n",
                "    layer_lstm(units = tau2,\n",
                "               activation = 'tanh',\n",
                "               recurrent_activation = 'tanh',\n",
                "               return_sequences = TRUE,\n",
                "               name = 'LSTM2'\n",
                "    ) %>%\n",
                "    layer_lstm(units = tau3,\n",
                "               activation = 'tanh',\n",
                "               recurrent_activation = 'tanh',\n",
                "               name = 'LSTM3'\n",
                "    ) %>%\n",
                "    layer_dense(units = 1,\n",
                "                activation = k_exp,\n",
                "                name = \"Output\",\n",
                "                weights = list(array(0, dim = c(tau3, 1)), \n",
                "                               array(log(y0), dim = c(1)))\n",
                "    )\n",
                "  model <- keras_model(inputs = list(Input), outputs = c(Output))\n",
                "  model %>% compile(loss = 'mean_squared_error', optimizer = optimizer)\n",
                "}\n",
                "\n",
                "# time-distributed layer on LSTMs\n",
                "LSTM_TD <- function(T0, tau0, tau1, y0 = 0, optimizer) {\n",
                "  Input <- layer_input(shape = c(T0, tau0),\n",
                "                       dtype = 'float32',\n",
                "                       name = 'Input')\n",
                "  Output <- Input %>%\n",
                "    layer_lstm(units = tau1,\n",
                "               activation = 'tanh',\n",
                "               recurrent_activation = 'tanh',\n",
                "               return_sequences = TRUE,\n",
                "               name = 'LSTM1'\n",
                "    ) %>%\n",
                "    time_distributed(layer_dense(units = 1,\n",
                "                                 activation = k_exp,\n",
                "                                 name = \"Output\"\n",
                "                    ),\n",
                "                    name = 'TD'\n",
                "    )\n",
                "  model <- keras_model(inputs = list(Input), outputs = c(Output))\n",
                "  model %>% compile(loss = 'mean_squared_error', optimizer = optimizer)\n",
                "}\n",
                "\n",
                "# single hidden GRU layer\n",
                "GRU1 <- function(T0, tau0, tau1, y0 = 0, optimizer) {\n",
                "  Input <- layer_input(shape = c(T0, tau0),\n",
                "                       dtype = 'float32',\n",
                "                       name = 'Input')\n",
                "  Output <- Input %>%\n",
                "    layer_gru(units = tau1,\n",
                "              activation = 'tanh',\n",
                "              recurrent_activation = 'tanh',\n",
                "              name = 'GRU1'\n",
                "    ) %>%\n",
                "    layer_dense(units = 1,\n",
                "                activation = k_exp,\n",
                "                name = \"Output\",\n",
                "                weights = list(array(0, dim = c(tau1, 1)), \n",
                "                               array(log(y0), dim = c(1)))\n",
                "    )\n",
                "  model <- keras_model(inputs = list(Input), outputs = c(Output))\n",
                "  model %>% compile(loss = 'mean_squared_error', optimizer = optimizer)\n",
                "}\n",
                "\n",
                "# double hidden GRU layers\n",
                "GRU2 <- function(T0, tau0, tau1, tau2, y0 = 0, optimizer) {\n",
                "  Input <- layer_input(shape = c(T0, tau0),\n",
                "                       dtype = 'float32',\n",
                "                       name = 'Input')\n",
                "  Output <- Input %>%\n",
                "    layer_gru(units = tau1,\n",
                "              activation = 'tanh',\n",
                "              recurrent_activation = 'tanh',\n",
                "              return_sequences = TRUE,\n",
                "              name = 'GRU1'\n",
                "    ) %>%\n",
                "    layer_gru(units = tau2,\n",
                "              activation = 'tanh',\n",
                "              recurrent_activation = 'tanh',\n",
                "              name = 'GRU2'\n",
                "    ) %>%\n",
                "    layer_dense(units = 1,\n",
                "                activation = k_exp,\n",
                "                name = \"Output\",\n",
                "                weights = list(array(0, dim = c(tau2, 1)), \n",
                "                               array(log(y0), dim = c(1)))\n",
                "    )\n",
                "  model <- keras_model(inputs = list(Input), outputs = c(Output))\n",
                "  model %>% compile(loss = 'mean_squared_error', optimizer = optimizer)\n",
                "}\n",
                "\n",
                "# triple hidden GRU layers\n",
                "GRU3 <- function(T0, tau0, tau1, tau2, tau3, y0 = 0, optimizer) {\n",
                "  Input <- layer_input(shape = c(T0, tau0),\n",
                "                       dtype = 'float32',\n",
                "                       name = 'Input')\n",
                "  Output <- Input %>%\n",
                "    layer_gru(units = tau1,\n",
                "              activation = 'tanh',\n",
                "              recurrent_activation = 'tanh',\n",
                "              return_sequences = TRUE,\n",
                "              name = 'GRU1'\n",
                "    ) %>%\n",
                "    layer_gru(units = tau2,\n",
                "              activation = 'tanh',\n",
                "              recurrent_activation = 'tanh',\n",
                "              return_sequences = TRUE,\n",
                "              name = 'GRU2'\n",
                "    ) %>%\n",
                "    layer_gru(units = tau3,\n",
                "              activation = 'tanh',\n",
                "              recurrent_activation = 'tanh',\n",
                "              name = 'GRU3'\n",
                "    ) %>%\n",
                "    layer_dense(units = 1,\n",
                "                activation = k_exp,\n",
                "                name = \"Output\",\n",
                "                weights = list(array(0, dim = c(tau3, 1)), \n",
                "                               array(log(y0), dim = c(1)))\n",
                "    )\n",
                "  model <- keras_model(inputs = list(Input), outputs = c(Output))\n",
                "  model %>% compile(loss = 'mean_squared_error', optimizer = optimizer)\n",
                "}\n",
                "\n",
                "# triple hidden LSTM layers both genders\n",
                "LSTM3.Gender <- function(T0, tau0, tau1, tau2, tau3, y0 = 0, optimizer) {\n",
                "    Input <- layer_input(shape = c(T0, tau0),\n",
                "                         dtype = 'float32',\n",
                "                         name = 'Input')\n",
                "    Gender <- layer_input(shape = c(1),\n",
                "                          dtype = 'float32',\n",
                "                          name = 'Gender')\n",
                "    RNN <- Input %>%\n",
                "      layer_lstm(units = tau1,\n",
                "                 activation = 'tanh',\n",
                "                 recurrent_activation = 'tanh',\n",
                "                 return_sequences = TRUE,\n",
                "                 name = 'LSTM1'\n",
                "      ) %>%\n",
                "      layer_lstm(units = tau2,\n",
                "                 activation = 'tanh',\n",
                "                 recurrent_activation = 'tanh',\n",
                "                 return_sequences = TRUE,\n",
                "                 name = 'LSTM2'\n",
                "      ) %>%\n",
                "      layer_lstm(units = tau3,\n",
                "                 activation = 'tanh',\n",
                "                 recurrent_activation = 'tanh',\n",
                "                 name = 'LSTM3'\n",
                "      )\n",
                "    Output <- list(RNN, Gender) %>% \n",
                "      layer_concatenate(name = \"Concat\") %>%\n",
                "      layer_dense(units = 1,\n",
                "                  activation = k_exp,\n",
                "                  name = \"Output\",\n",
                "                  weights = list(array(0, dim = c(tau3 + 1, 1)), \n",
                "                                 array(log(y0), dim = c(1)))\n",
                "      )\n",
                "    model <- keras_model(inputs = list(Input, Gender), outputs = c(Output))\n",
                "    model %>% compile(loss = 'mean_squared_error', optimizer = optimizer)\n",
                "  }\n",
                "\n",
                "# triple hidden GRU layers both genders\n",
                "GRU3.Gender <-\n",
                "  function(T0, tau0, tau1, tau2, tau3, y0 = 0, optimizer) {\n",
                "    Input <- layer_input(shape = c(T0, tau0),\n",
                "                         dtype = 'float32',\n",
                "                         name = 'Input')\n",
                "    Gender <- layer_input(shape = c(1),\n",
                "                          dtype = 'float32',\n",
                "                          name = 'Gender')\n",
                "    RNN <- Input %>%\n",
                "      layer_gru(units = tau1,\n",
                "                activation = 'tanh',\n",
                "                recurrent_activation = 'tanh',\n",
                "                return_sequences = TRUE,\n",
                "                name = 'GRU1'\n",
                "      ) %>%\n",
                "      layer_gru(units = tau2,\n",
                "                activation = 'tanh',\n",
                "                recurrent_activation = 'tanh',\n",
                "                return_sequences = TRUE,\n",
                "                name = 'GRU2'\n",
                "      ) %>%\n",
                "      layer_gru(units = tau3,\n",
                "                activation = 'tanh',\n",
                "                recurrent_activation = 'tanh',\n",
                "                name = 'GRU3'\n",
                "      )\n",
                "    Output <- list(RNN, Gender) %>% layer_concatenate(name = \"Concat\") %>%\n",
                "      layer_dense(units = 1,\n",
                "                  activation = k_exp,\n",
                "                  name = \"Output\",\n",
                "                  weights = list(array(0, dim = c(tau3 + 1, 1)),\n",
                "                                 array(log(y0), dim = c(1)))\n",
                "      )\n",
                "    model <- keras_model(inputs = list(Input, Gender), outputs = c(Output))\n",
                "    model %>% compile(loss = 'mean_squared_error', optimizer = optimizer)\n",
                "  }\n",
                "\n",
                "# double hidden FNN layers\n",
                "FNN <- function(T0, tau0, tau1, tau2, y0 = 0, optimizer) {\n",
                "  Input <- layer_input(shape = c(T0, tau0),\n",
                "                       dtype = 'float32',\n",
                "                       name = 'Input')\n",
                "  Output <-Input %>% \n",
                "    layer_reshape(target_shape = c(T0 * tau0), \n",
                "                  name = 'Reshape') %>%\n",
                "    layer_dense(units = tau1,\n",
                "                activation = 'tanh',\n",
                "                name = 'Layer1') %>%\n",
                "    layer_dense(units = tau2,\n",
                "                activation = 'tanh',\n",
                "                name = 'Layer2') %>%\n",
                "    layer_dense(units = 1,\n",
                "                activation = k_exp,\n",
                "                name = \"Output\",\n",
                "                weights = list(array(0, dim = c(tau2, 1)), \n",
                "                               array(log(y0), dim = c(1)))\n",
                "    )\n",
                "  model <- keras_model(inputs = list(Input), outputs = c(Output))\n",
                "  model %>% compile(loss = 'mean_squared_error', optimizer = optimizer)\n",
                "}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##################################################################\n",
                "### LSTMs and GRUs\n",
                "##################################################################\n",
                "\n",
                "T0 <- 10\n",
                "tau0 <- 3\n",
                "tau1 <- 5\n",
                "tau2 <- 4\n",
                "\n",
                "summary(LSTM1(T0, tau0, tau1, 0, \"nadam\"))\n",
                "summary(LSTM2(T0, tau0, tau1, tau2, 0, \"nadam\"))\n",
                "summary(LSTM_TD(T0, tau0, tau1, 0, \"nadam\"))\n",
                "summary(GRU1(T0, tau0, tau1, 0, \"nadam\"))\n",
                "summary(GRU2(T0, tau0, tau1, tau2, 0, \"nadam\"))\n",
                "summary(FNN(T0, tau0, tau1, tau2, 0, \"nadam\"))\n",
                "\n",
                "\n",
                "##################################################################\n",
                "### Bringing the data in the right structure for a toy example\n",
                "##################################################################\n",
                "\n",
                "gender <- \"Female\"\n",
                "ObsYear <- 2000\n",
                "\n",
                "mort_rates <- all_mort[which(all_mort$Gender == gender), \n",
                "                      c(\"Year\", \"Age\", \"logmx\")]\n",
                "mort_rates <- dcast(mort_rates, Year ~ Age, value.var = \"logmx\")\n",
                "\n",
                "T0 <- 10     # lookback period\n",
                "tau0 <- 3    # dimension of x_t (should be odd for our application)\n",
                "delta0 <- (tau0 - 1) / 2\n",
                "\n",
                "toy_rates <- as.matrix(\n",
                "  mort_rates[which(mort_rates$Year %in% c((ObsYear - T0):(ObsYear + 1))),])\n",
                "\n",
                "xt <- array(NA, c(2, ncol(toy_rates) - tau0, T0, tau0))\n",
                "YT <- array(NA, c(2, ncol(toy_rates) - tau0))\n",
                "\n",
                "for (i in 1:2) {\n",
                "  for (a0 in 1:(ncol(toy_rates) - tau0)) {\n",
                "    xt[i, a0, , ] <- toy_rates[c(i:(T0 + i - 1)), c((a0 + 1):(a0 + tau0))]\n",
                "    YT[i, a0] <- toy_rates[T0 + i, a0 + 1 + delta0]\n",
                "  }\n",
                "}\n",
                "\n",
                "plot(x = toy_rates[1:T0, 1],\n",
                "     y = toy_rates[1:T0, 2],\n",
                "     col = \"white\",\n",
                "     xlab = \"calendar years\",\n",
                "     ylab = \"raw log-mortality rates\",\n",
                "     cex.lab = 1.5,\n",
                "     cex = 1.5,\n",
                "     main = list(\"data toy example\", cex = 1.5),\n",
                "     xlim = range(toy_rates[, 1]),\n",
                "     ylim = range(toy_rates[, -1]),\n",
                "     type = 'l'\n",
                ")\n",
                "\n",
                "for (a0 in 2:ncol(toy_rates)) {\n",
                "  if (a0 %in% (c(1:100) * 3)) {\n",
                "    lines(x = toy_rates[1:T0, 1], y = toy_rates[1:T0, a0])\n",
                "    points(x = toy_rates[(T0 + 1):(T0 + 2), 1],\n",
                "           y = toy_rates[(T0 + 1):(T0 + 2), a0],\n",
                "           col = c(\"blue\", \"red\"),\n",
                "           pch = 20\n",
                "    )\n",
                "    lines(x = toy_rates[(T0):(T0 + 1), 1],\n",
                "          y = toy_rates[(T0):(T0 + 1), a0],\n",
                "          col = \"blue\",\n",
                "          lty = 2\n",
                "    )\n",
                "    lines(x = toy_rates[(T0 + 1):(T0 + 2), 1],\n",
                "          y = toy_rates[(T0 + 1):(T0 + 2), a0],\n",
                "          col = \"red\",\n",
                "          lty = 2\n",
                "    )\n",
                "  }\n",
                "}\n",
                "\n",
                "\n",
                "##################################################################\n",
                "### LSTMs and GRUs\n",
                "##################################################################\n",
                "\n",
                "x.train <- array(2 * (xt[1, , , ] - min(xt)) / (max(xt) - min(xt)) - 1, \n",
                "                 c(ncol(toy_rates) - tau0, T0, tau0))\n",
                "x.test  <- array(2 * (xt[2, , , ] - min(xt)) / (max(xt) - min(xt)) - 1, \n",
                "                 c(ncol(toy_rates) - tau0, T0, tau0))\n",
                "y.train <- -YT[1, ]\n",
                "(y0 <- mean(y.train))\n",
                "y.test  <- -YT[2, ]\n",
                "\n",
                "### examples\n",
                "\n",
                "tau1 <- 5    # dimension of the outputs z_t^(1) first RNN layer\n",
                "tau2 <- 4    # dimension of the outputs z_t^(2) second RNN layer\n",
                "\n",
                "CBs <- callback_model_checkpoint(\"./CallBack/best_model\",\n",
                "                                 monitor = \"val_loss\",\n",
                "                                 verbose = 0,\n",
                "                                 save_best_only = TRUE,\n",
                "                                 save_weights_only = TRUE\n",
                ")\n",
                "model <- LSTM2(T0, tau0, tau1, tau2, y0, \"nadam\")\n",
                "summary(model)\n",
                "\n",
                "# DM: takes 40 seconds on my laptop\n",
                "# expected run-time on Renku 8GB environment around 120 seconds\n",
                "system.time(\n",
                "  fit <- model %>% fit(x = x.train,\n",
                "                       y = y.train,\n",
                "                       validation_data = list(x.test, y.test),\n",
                "                       batch_size = 10,\n",
                "                       epochs = 500,\n",
                "                       verbose = 0,\n",
                "                       callbacks = CBs\n",
                "                   )\n",
                ")\n",
                "\n",
                "plot(fit[[2]]$val_loss,\n",
                "     col = \"red\",\n",
                "     ylim = c(0, 0.2),\n",
                "     main = list(\"early stopping rule\", cex = 1.5),\n",
                "     xlab = \"epochs\",\n",
                "     ylab = \"MSE loss\",\n",
                "     cex = 1.5,\n",
                "     cex.lab = 1.5\n",
                ")\n",
                "lines(fit[[2]]$loss, col = \"blue\")\n",
                "abline(h = 0.1, lty = 1, col = \"black\")\n",
                "legend(x = \"bottomleft\",\n",
                "       col = c(\"blue\", \"red\"),\n",
                "       lty = c(1, -1),\n",
                "       lwd = c(1, -1),\n",
                "       pch = c(-1, 1),\n",
                "       legend = c(\"in-sample loss\", \"out-of-sample loss\")\n",
                ")\n",
                "\n",
                "\n",
                "load_model_weights_hdf5(model, \"./CallBack/best_model\")\n",
                "Yhat.train1 <- as.vector(model %>% predict(x.train))\n",
                "Yhat.test1 <- as.vector(model %>% predict(x.test))\n",
                "c(round(mean((Yhat.train1 - y.train) ^ 2), 4), \n",
                "  round(mean((Yhat.test1 - y.test) ^ 2), 4))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##################################################################\n",
                "### prepare data for Recurrent Neural Networks \n",
                "##################################################################\n",
                "\n",
                "data.preprocessing.RNNs <- function(data.raw, gender, T0,\n",
                "                                    tau0, ObsYear = 1999) {\n",
                "    mort_rates <- data.raw[which(data.raw$Gender == gender), \n",
                "                           c(\"Year\", \"Age\", \"logmx\")]\n",
                "    mort_rates <- dcast(mort_rates, Year ~ Age, value.var = \"logmx\")\n",
                "    # selecting data\n",
                "    train.rates <- as.matrix(mort_rates[which(mort_rates$Year <= ObsYear), ])\n",
                "    # adding padding at the border\n",
                "    (delta0 <- (tau0 - 1) / 2)\n",
                "    if (delta0 > 0) {\n",
                "      for (i in 1:delta0) {\n",
                "        train.rates <- as.matrix(cbind(train.rates[, 1], \n",
                "                                      train.rates[, 2], \n",
                "                                      train.rates[, -1], \n",
                "                                      train.rates[, ncol(train.rates)]))\n",
                "      }\n",
                "    }\n",
                "    train.rates <- train.rates[, -1]\n",
                "    (t1 <- nrow(train.rates) - (T0 - 1) - 1)\n",
                "    (a1 <- ncol(train.rates) - (tau0 - 1))\n",
                "    (n.train <- t1 * a1) # number of training samples\n",
                "    xt.train <- array(NA, c(n.train, T0, tau0))\n",
                "    YT.train <- array(NA, c(n.train))\n",
                "    for (t0 in (1:t1)) {\n",
                "      for (a0 in (1:a1)) {\n",
                "        xt.train[(t0 - 1) * a1 + a0, , ] <- train.rates[t0:(t0 + T0 - 1),\n",
                "                                                        a0:(a0 + tau0 - 1)]\n",
                "        YT.train[(t0 - 1) * a1 + a0] <- train.rates[t0 + T0, a0 + delta0]\n",
                "      }\n",
                "    }\n",
                "    list(xt.train, YT.train)\n",
                "}\n",
                "\n",
                "\n",
                "##################################################################\n",
                "### recursive prediction\n",
                "##################################################################\n",
                "\n",
                "recursive.prediction.internal <- function(ObsYear, all_mort2, gender, T0, tau0,\n",
                "                                          x.min, x.max, model.p, set_gender) {\n",
                "    single.years <- array(NA, c(2016 - ObsYear))\n",
                "    \n",
                "    for (ObsYear1 in ((ObsYear + 1):2016)) {\n",
                "      data2 <- data.preprocessing.RNNs(\n",
                "        all_mort2[which(all_mort2$Year >= (ObsYear1 - 10)), ],\n",
                "        gender, T0, tau0, ObsYear1)\n",
                "      # MinMaxScaler (with minimum and maximum from above)\n",
                "      x.test <- array(2 * (data2[[1]] - x.min) / (x.min - x.max) - 1,\n",
                "                      dim(data2[[1]]))\n",
                "      if (set_gender) {\n",
                "        if (gender == \"Female\") {\n",
                "          yy <- 0\n",
                "        } else {\n",
                "          yy <- 1\n",
                "        }\n",
                "        x.test <- list(x.test, rep(yy, dim(x.test)[1]))\n",
                "      }\n",
                "      y.test <- -data2[[2]]\n",
                "      Yhat.test2 <- exp(-as.vector(model.p %>% predict(x.test)))\n",
                "      single.years[ObsYear1 - ObsYear] <- round(10 ^ 4 * mean((Yhat.test2 -exp(-y.test)) ^ 2), 4)\n",
                "      predicted <- all_mort2[which(all_mort2$Year == ObsYear1), ]\n",
                "      keep <- all_mort2[which(all_mort2$Year != ObsYear1), ]\n",
                "      predicted$logmx <- -as.vector(model %>% predict(x.test))\n",
                "      predicted$mx <- exp(predicted$logmx)\n",
                "      all_mort2 <- rbind(keep, predicted)\n",
                "      all_mort2 <- all_mort2[order(Gender, Year, Age), ]\n",
                "    }\n",
                "    list(all_mort2, single.years)\n",
                "}\n",
                "\n",
                "recursive.prediction <- function(ObsYear, all_mort2, gender, T0, tau0, x.min,\n",
                "                                 x.max, model.p) {\n",
                "    recursive.prediction.internal(ObsYear, all_mort2,gender, T0, tau0, x.min,\n",
                "                                  x.max, model.p, FALSE)\n",
                "}\n",
                "\n",
                "\n",
                "recursive.prediction.Gender <- function(ObsYear, all_mort2, gender, T0,\n",
                "                                        tau0,x.min, x.max, model.p) {\n",
                "    recursive.prediction.internal(ObsYear, all_mort2, gender, T0, tau0, x.min,\n",
                "                                  x.max, model.p, TRUE)\n",
                "}\n",
                "\n",
                "##################################################################\n",
                "### plotting functions\n",
                "##################################################################\n",
                "\n",
                "plot.losses <- function(name.model, gender, val_loss, loss) {\n",
                "  plot(val_loss,\n",
                "       col = \"cyan3\",\n",
                "       pch = 20,\n",
                "       ylim = c(0, 0.1),\n",
                "       main = list(\n",
                "         paste(\"early stopping: \", name.model, \", \", gender, sep = \"\"),\n",
                "         cex = 1.5\n",
                "       ),\n",
                "       xlab = \"epochs\",\n",
                "       ylab = \"MSE loss\",\n",
                "       cex = 1.5,\n",
                "       cex.lab = 1.5\n",
                "  )\n",
                "  lines(loss, col = \"blue\")\n",
                "  abline(h = 0.05, lty = 1, col = \"black\")\n",
                "  legend(x = \"bottomleft\",\n",
                "         col = c(\"blue\", \"cyan3\"),\n",
                "         lty = c(1, -1),\n",
                "         lwd = c(1, -1),\n",
                "         pch = c(-1, 20),\n",
                "         legend = c(\"in-sample loss\", \"out-of-sample loss\")\n",
                "  )\n",
                "}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##################################################################\n",
                "### Setting the parameters and load data\n",
                "##################################################################\n",
                "\n",
                "\n",
                "# choice of parameters\n",
                "T0 <- 10\n",
                "tau0 <- 5\n",
                "gender <- \"Female\"\n",
                "ObsYear <- 1999\n",
                "\n",
                "# training data pre-processing\n",
                "data1 <- data.preprocessing.RNNs(all_mort, gender, T0, tau0, ObsYear)\n",
                "\n",
                "# validation data pre-processing\n",
                "all_mort2 <- all_mort[which((all_mort$Year > (ObsYear - 10)) & \n",
                "                              (Gender == gender)), ]\n",
                "all_mortV <- all_mort2\n",
                "test.Y <- all_mortV[which(all_mortV$Year > ObsYear), ]\n",
                "\n",
                "# MinMaxScaler data pre-processing\n",
                "x.min <- min(data1[[1]])\n",
                "x.max <- max(data1[[1]])\n",
                "x.train <- array(2 * (data1[[1]] - x.min) / (x.min - x.max) - 1,\n",
                "                 dim(data1[[1]]))\n",
                "y.train <- -data1[[2]]\n",
                "y0 <- mean(y.train)\n",
                "\n",
                "##################################################################\n",
                "### LSTM architectures\n",
                "##################################################################\n",
                "\n",
                "# network architecture deep 3 network\n",
                "tau1 <- 20\n",
                "tau2 <- 15\n",
                "tau3 <- 10\n",
                "optimizer <- 'adam'\n",
                "\n",
                "# choose either LSTM or GRU network\n",
                "RNN.type <- \"LSTM\"\n",
                "#RNN.type <- \"GRU\"\n",
                "\n",
                "if (RNN.type == \"LSTM\") {\n",
                "  model <- LSTM3(T0, tau0, tau1, tau2, tau3, y0, optimizer)\n",
                "} else {\n",
                "  model <- GRU3(T0, tau0, tau1, tau2, tau3, y0, optimizer)\n",
                "}\n",
                "name.model <- paste(RNN.type, \"3_\", tau0, \"_\", tau1, \"_\", tau2, \"_\", tau3, \n",
                "                    sep = \"\")\n",
                "file.name <- paste(\"./CallBack/best_model_\", name.model, \"_\", gender, sep = \"\")\n",
                "summary(model)\n",
                "\n",
                "# define callback\n",
                "CBs <- callback_model_checkpoint(file.name,\n",
                "                                 monitor = \"val_loss\",\n",
                "                                 verbose = 0,\n",
                "                                 save_best_only = TRUE,\n",
                "                                 save_weights_only = TRUE\n",
                ")\n",
                "\n",
                "# DM: gradient descent fitting: takes roughly 200 seconds on my laptop (500 epochs)\n",
                "# n_epochs <- 500\n",
                "n_epochs <- 100\n",
                "# expected run-time on Renku 8GB environment around 60 seconds (100 epochs)\n",
                "system.time(\n",
                "  fit <- model %>% fit(x = x.train,\n",
                "                       y = y.train,\n",
                "                       validation_split = 0.2,\n",
                "                       batch_size = 100,\n",
                "                       epochs = n_epochs,\n",
                "                       verbose = 0,\n",
                "                       callbacks = CBs\n",
                "                   )\n",
                ")\n",
                "\n",
                "# plot loss figures\n",
                "plot.losses(name.model, gender, fit[[2]]$val_loss, fit[[2]]$loss)\n",
                "\n",
                "# calculating in-sample loss: LC is c(Female=3.7573, Male=8.8110)\n",
                "load_model_weights_hdf5(model, file.name)\n",
                "round(10 ^ 4 * mean((exp(-as.vector(model %>% predict(x.train))) -\n",
                "                       exp(-y.train)) ^ 2), 4)\n",
                "\n",
                "# calculating out-of-sample loss: LC is c(Female=0.6045, Male=1.8152)\n",
                "pred.result <- recursive.prediction(ObsYear, all_mort2, gender, T0, tau0,\n",
                "                                    x.min, x.max, model)\n",
                "test <- pred.result[[1]][which(all_mort2$Year > ObsYear), ]\n",
                "round(10 ^ 4 * mean((test$mx - test.Y$mx) ^ 2), 4)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##################################################################\n",
                "### Setting the parameters and load data\n",
                "##################################################################\n",
                "\n",
                "# choice of parameters\n",
                "T0 <- 10\n",
                "tau0 <- 5\n",
                "ObsYear <- 1999\n",
                "\n",
                "# training data pre-processing\n",
                "data1 <- data.preprocessing.RNNs(all_mort, \"Female\", T0, tau0, ObsYear)\n",
                "data2 <- data.preprocessing.RNNs(all_mort, \"Male\", T0, tau0, ObsYear)\n",
                "\n",
                "xx <- dim(data1[[1]])[1]\n",
                "x.train <- array(NA, dim = c(2 * xx, dim(data1[[1]])[c(2, 3)]))\n",
                "y.train <- array(NA, dim = c(2 * xx))\n",
                "gender.indicator <- rep(c(0, 1), xx)\n",
                "for (l in 1:xx) {\n",
                "  x.train[(l - 1) * 2 + 1, ,] <- data1[[1]][l, ,]\n",
                "  x.train[(l - 1) * 2 + 2, ,] <- data2[[1]][l, ,]\n",
                "  y.train[(l - 1) * 2 + 1] <- -data1[[2]][l]\n",
                "  y.train[(l - 1) * 2 + 2] <- -data2[[2]][l]\n",
                "}\n",
                "# MinMaxScaler data pre-processing\n",
                "x.min <- min(x.train)\n",
                "x.max <- max(x.train)\n",
                "x.train <- list(array(2 * (x.train - x.min) / (x.min - x.max) - 1,\n",
                "                      dim(x.train)), \n",
                "                gender.indicator)\n",
                "y0 <- mean(y.train)\n",
                "\n",
                "\n",
                "# validation data pre-processing\n",
                "all_mort2.Female <- all_mort[which((all_mort$Year > (ObsYear - 10)) &\n",
                "                                     (Gender == \"Female\")), ]\n",
                "test.Y.Female <- all_mort2.Female[which(all_mort2.Female$Year > ObsYear), ]\n",
                "all_mort2.Male <- all_mort[which((all_mort$Year > (ObsYear - 10)) &\n",
                "                                   (Gender == \"Male\")), ]\n",
                "test.Y.Male <- all_mort2.Male[which(all_mort2.Male$Year > ObsYear), ]\n",
                "\n",
                "\n",
                "##################################################################\n",
                "### LSTM architectures\n",
                "##################################################################\n",
                "\n",
                "# network architecture deep 3 network\n",
                "tau1 <- 20\n",
                "tau2 <- 15\n",
                "tau3 <- 10\n",
                "optimizer <- 'adam'\n",
                "\n",
                "# choose either LSTM or GRU network\n",
                "RNN.type <- \"LSTM\"\n",
                "#RNN.type <- \"GRU\"\n",
                "\n",
                "if (RNN.type == \"LSTM\") {\n",
                "  model <- LSTM3.Gender(T0, tau0, tau1, tau2, tau3, y0, optimizer)\n",
                "} else {\n",
                "  model <- GRU3.Gender(T0, tau0, tau1, tau2, tau3, y0, optimizer)\n",
                "}\n",
                "name.model <- paste(RNN.type, \"3_\", tau0, \"_\", tau1, \"_\", tau2, \"_\", tau3,\n",
                "                    sep = \"\")\n",
                "#file.name <- paste(\"./Model_Full_Param/best_model_\", name.model, sep=\"\")\n",
                "file.name <- paste(\"./CallBack/best_model_\", name.model, sep = \"\")\n",
                "summary(model)\n",
                "\n",
                "# define callback\n",
                "CBs <- callback_model_checkpoint(file.name,\n",
                "                                 monitor = \"val_loss\",\n",
                "                                 verbose = 0,\n",
                "                                 save_best_only = TRUE,\n",
                "                                 save_weights_only = TRUE\n",
                ")\n",
                "\n",
                "# DM: gradient descent fitting: takes roughly 400 seconds on my laptop (500 epochs)\n",
                "# n_epochs <- 500\n",
                "n_epochs <- 100\n",
                "# expected run-time on Renku 8GB environment around 115 seconds (100 epochs)\n",
                "system.time(\n",
                "  fit <- model %>% fit(x = x.train,\n",
                "                       y = y.train,\n",
                "                       validation_split = 0.2,\n",
                "                       batch_size = 100,\n",
                "                       epochs = n_epochs,\n",
                "                       verbose = 0,\n",
                "                       callbacks = CBs\n",
                "  )\n",
                ")\n",
                "\n",
                "# plot loss figures\n",
                "plot.losses(name.model, \"Both\", fit[[2]]$val_loss, fit[[2]]$loss)\n",
                "\n",
                "# calculating in-sample loss: LC is c(Female=3.7573, Male=8.8110)\n",
                "load_model_weights_hdf5(model, file.name)\n",
                "round(10 ^ 4 * mean((exp(-as.vector(model %>% predict(x.train))) -\n",
                "                       exp(-y.train)) ^ 2), 4)\n",
                "\n",
                "# calculating out-of-sample loss: LC is c(Female=0.6045, Male=1.8152)\n",
                "# Female\n",
                "pred.result <- recursive.prediction.Gender(ObsYear, all_mort2.Female, \"Female\",\n",
                "                                          T0, tau0, x.min, x.max, model)\n",
                "test <- pred.result[[1]][which(all_mort2.Female$Year > ObsYear), ]\n",
                "round(10 ^ 4 * mean((test$mx - test.Y.Female$mx) ^ 2), 4)\n",
                "# Male\n",
                "pred.result <- recursive.prediction.Gender(ObsYear, all_mort2.Male, \"Male\", T0,\n",
                "                                           tau0, x.min, x.max, model)\n",
                "test <- pred.result[[1]][which(all_mort2.Male$Year > ObsYear), ]\n",
                "round(10 ^ 4 * mean((test$mx - test.Y.Male$mx) ^ 2), 4)\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
